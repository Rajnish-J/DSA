# Time Complexity

## Comparison:

| Old Computer | Brand New Computer |
|-------------|------------------|
| **Task:** Find an unavailable number in an array of size **1 million** | **Same Task** |
| **Time Taken:** 10 seconds | **Time Taken:** 1 second |

ğŸ”¹ **Conclusion:** Even though the new machine is faster, both machines have the **same time complexity**. **Time complexity â‰  Actual Time Taken**.

---

## ğŸ“Œ **Definition**
Time complexity is a **mathematical function** that describes how execution time **grows** as input size increases. It tells us the relationship between **input size** and **time taken**.

> "Time Complexity is a function that defines how the execution time increases as the input size grows."

### ğŸ” Why Do We Need Time Complexity?
Let's compare **Linear Search** and **Binary Search**:

- **Linear Search**: O(N)
- **Binary Search**: O(log N)

> âœ… **Key Tip:** Always think about **large inputs**. Ignore small inputs when analyzing complexity.

### Graph Comparison:

```plaintext
    Complexity Growth Chart
    (Y-axis: Time | X-axis: Input Size)

    O(1)   ----
    O(log N)  --/--
    O(N)     ----/----
    O(N^2)   ----/----/----
    O(2^N)   ----/----/----/----
```

ğŸ‘‰ **Conclusion:** O(1) < O(log N) < O(N) < O(NÂ²) < O(2^N)

---

## ğŸ“Œ **Key Considerations When Analyzing Complexity**

1ï¸âƒ£ Always analyze the **worst-case** complexity.
2ï¸âƒ£ Always consider the **complexity for large inputs**.
3ï¸âƒ£ Ignore **constants and less dominant terms** because they don't significantly affect large inputs.

#### Example:
Given complexity:

```plaintext
O(NÂ³ + log N)
```

For **N = 1,000,000 (1 million):**

```plaintext
(1,000,000)Â³ + log(1,000,000)
= (1,000,000)Â³ + 6 seconds (since log(1M) â‰ˆ 6)
```

Since **1 millionÂ³ is much larger than 6**, we ignore `log N`.

ğŸ‘‰ **Final Complexity** â†’ **O(NÂ³)**

---

## ğŸ“Œ **Common Time Complexities & Examples**

| Complexity | Notation | Example |
|------------|---------|---------|
| Constant Time | O(1) | Array index access `arr[i]` |
| Logarithmic Time | O(log N) | Binary search |
| Linear Time | O(N) | Looping through an array |
| Linearithmic Time | O(N log N) | Merge sort, Quick sort (average case) |
| Quadratic Time | O(NÂ²) | Nested loops (e.g., Bubble Sort) |
| Cubic Time | O(NÂ³) | Triple nested loops |
| Exponential Time | O(2^N) | Recursive Fibonacci |
| Factorial Time | O(N!) | Traveling Salesman Problem |

---

## ğŸ“Œ **Big O Notation (O)**

- Represents the **upper bound** (worst-case scenario) of an algorithm.
- Ensures that the function does **not exceed** a certain growth rate.
- Example: If an algorithm runs in **O(NÂ²)**, it means that for large **N**, the function's time complexity won't grow beyond **NÂ²**.

```plaintext
O(NÂ²) means:    0  < lim f(N) < âˆ
                      N -> âˆ    g(N)
```

---

## ğŸ“Œ **Big Omega Notation (Î©)**

- Represents the **lower bound** (best-case scenario) of an algorithm.
- Guarantees that the function takes at **least** the given complexity.
- Example: If an algorithm runs in **Î©(N)**, it means the algorithm's execution time **won't be less** than **N**.

```plaintext
Î©(N) means the algorithm **at least** takes N time.
```

---

## ğŸ“Œ **Theta Notation (Î˜)**

- When an algorithm has **both upper and lower bounds** equal to the same function.
- Example: If an algorithm runs in **O(NÂ²) and Î©(NÂ²)**, then it is **Î˜(NÂ²)**.

```plaintext
Î˜(NÂ²) = O(NÂ²) + Î©(NÂ²)
```

---

## ğŸ“Œ **Little o Notation (o)**

- Also gives an **upper bound**, but it's a strict upper bound (i.e., the algorithm's growth rate is **less than** the given function but **never equal** to it).
- Example: `o(NÂ²)` means that for large **N**, the function's execution time **grows slower** than **NÂ²**, but never exactly at **NÂ²**.

```plaintext
o(NÂ²) means f(N) grows strictly slower than NÂ².
```

---

## ğŸ“Œ **Example Code Implementations**

### 1ï¸âƒ£ **O(1) - Constant Time**
```python
# Accessing an element in an array is O(1)
arr = [10, 20, 30, 40, 50]
print(arr[2])  # Always takes the same time
```

### 2ï¸âƒ£ **O(N) - Linear Time**
```python
# Looping through an array is O(N)
def find_element(arr, target):
    for num in arr:
        if num == target:
            return True
    return False
```

### 3ï¸âƒ£ **O(log N) - Logarithmic Time**
```python
# Binary Search Algorithm (O(log N))
def binary_search(arr, target):
    left, right = 0, len(arr) - 1
    while left <= right:
        mid = (left + right) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    return -1
```

### 4ï¸âƒ£ **O(NÂ²) - Quadratic Time**
```python
# Bubble Sort (O(NÂ²))
def bubble_sort(arr):
    n = len(arr)
    for i in range(n):
        for j in range(n - i - 1):
            if arr[j] > arr[j + 1]:
                arr[j], arr[j + 1] = arr[j + 1], arr[j]
```

---

## **Final Thoughts**
ğŸ‘‰ **Mastering time complexity helps in writing optimized code!**

